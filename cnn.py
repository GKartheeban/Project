# -*- coding: utf-8 -*-
"""cnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TMX4yENIlhhIYamfdmd9pyXAi13NFb_d

# first import libraries

from google.colab import drive
drive.mount('/content/drive')
"""

import pandas as  pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import os
import glob as gb
import keras
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout
from keras.layers import BatchNormalization
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam, SGD
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir("/content/drive/")
!ls

"""
# check the Train folder"""

import os
import glob as gb

trainpath = '/content/drive/MyDrive/dl_project/Training'

for folder in os.listdir(trainpath):
    files = gb.glob(pathname=os.path.join(trainpath, folder, '*.jpg'))
    print(f'For training data, found {len(files)} in folder {folder}')

"""# check the test folder"""

import os
import glob as gb

testpath = '/content/drive/MyDrive/dl_project/Testing'

for folder in os.listdir(trainpath):
    files = gb.glob(pathname=os.path.join(testpath, folder, '*.jpg'))
    print(f'For training data, found {len(files)} in folder {folder}')

code = {'glioma' : 0 , 'meningioma' : 1, 'notumor' : 2 , 'pituitary' : 3  }
#create a dictionary with their names & index , also create a function to get the index back
def convert_code (k):
    for x , y in code.items():
        if y == k :
            return x

import os
import cv2
import glob as gb
X_train = []
y_train = []

for folder in os.listdir(os.path.join(trainpath)):
    folder_path = os.path.join(trainpath, folder)
    files = gb.glob(os.path.join(folder_path, '*.jpg'))

    for file in files:
        image = cv2.imread(file)
        X_train.append(image)
        y_train.append(code[folder])

print(f'we have {len(X_train)} items in X_train')

""" we have have a look to random pictures in X_train , and to adjust their title using the y valu"""

plt.figure(figsize=(16,16))
for ax , i in enumerate(list(np.random.randint(0,len(X_train),16))) :
    plt.subplot(4,4,ax+1)
    plt.imshow(X_train[i])
    plt.axis('off')
    plt.title('Label: '+convert_code(y_train[i]))

X_test = [] # testing Dataset
y_test = [] # testing Labels
for folder in  os.listdir(trainpath) :
    files = gb.glob(pathname= str( trainpath +'Training//' + folder + '/*.jpg'))
    for file in files:
        image = cv2.imread(file)
        X_train.append(list(image))
        y_train.append(code[folder])

"""we have have a look to random pictures in x_test , and to adjust their title using the y valu"""

plt.figure(figsize=(16,16))
for ax , i in enumerate(list(np.random.randint(0,len(X_train),16))) :
    plt.subplot(4,4,ax+1)
    plt.imshow(X_train[i])
    plt.axis('off')
    plt.title('Label: '+convert_code(y_train[i]))

"""# Build CNN model"""

model = Sequential()

# Convolutional layer 1
model.add(Conv2D(64,(7,7), input_shape=(200, 200, 1), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

#Convolutional layer 2
model.add(Conv2D(128,(7,7), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

# Convolutional layer 3
model.add(Conv2D(128,(7,7), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

# Convolutional layer 4
model.add(Conv2D(256,(7,7), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

 # Convolutional layer 5
model.add(Conv2D(256,(7,7), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

# Convolutional layer 6
model.add(Conv2D(512,(7,7), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Flatten())

# Full connect layers

model.add(Dense(units= 1024, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(units=512, activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(units=4, activation='softmax'))

model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy',
                   metrics= ['categorical_accuracy'])

model.save('/content/drive/MyDrive/Deep learning/GK12.h5')

generator_train = ImageDataGenerator(rescale=1./255)

generator_test = ImageDataGenerator(rescale=1./255)

s = 200

train = generator_train.flow_from_directory('/content/drive/MyDrive/dl_project/Training', target_size=(s, s),batch_size=32, class_mode= "categorical", color_mode='grayscale')

test = generator_test.flow_from_directory('/content/drive/MyDrive/dl_project/Testing', target_size=(s, s),
                                              batch_size=32, class_mode= "categorical", color_mode='grayscale')

model.summary()

from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# Define callback functions
early_stopping = EarlyStopping(monitor='loss', min_delta=1e-11, patience=6, verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=6, verbose=1)
model_checkpoint = ModelCheckpoint(filepath='/content/drive/MyDrive/Deep learning/GK12.h5', monitor='val_categorical_accuracy',
                                    save_best_only=True, verbose=1)

# Train the model with callbacks
history = model.fit(train, epochs=5, validation_data=test, validation_steps=1311 // 32,
                    callbacks=[early_stopping, reduce_lr, model_checkpoint])

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss','val_loss']].plot()
history_frame.loc[:, ['categorical_accuracy','val_categorical_accuracy']].plot();

"""# evaluate the model in real"""

from tensorflow.keras.models import load_model



new_model= load_model('/content/drive/MyDrive/Deep learning/GK12.h5')

from tensorflow.keras.preprocessing.image import load_img, img_to_array

import numpy as np

# Load the image
img_path = "/content/drive/MyDrive/dl_project/Testing/glioma/Te-gl_0010.jpg"
img = load_img(img_path, target_size=(200, 200), grayscale=True)

# Convert the image to a numpy array
x = img_to_array(img)
x = np.expand_dims(x, axis=0)

# Normalize the pixel values
x = x / 255.0

# Make the prediction
prediction = new_model.predict(x)
from sklearn.metrics import accuracy_score


# Get the class label with the highest probability
predicted_class = np.argmax(prediction)
accuracy_score(predicted_class)

# Print the predicted tumor type
if predicted_class == 0:
    print("Glioma")
elif predicted_class == 1:
    print("Meningioma")
elif predicted_class == 2:
    print("No tumor")
elif predicted_class == 3:
    print("Pituitary")
else:
    print("Unknown class label")

